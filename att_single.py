"""
ç·¨è¼¯å¾®èª¿è…³æœ¬ - å®Œå…¨ä¿®æ­£ç‰ˆ v3
ä¸»è¦ä¿®æ­£ï¼š
1. å¢å¼· Prompt å·®ç•°ï¼ˆæé«˜ drop_ratioï¼Œå¢åŠ æ›´å¤šå±¬æ€§ï¼‰
2. ä¿®æ­£ v_target è¨ˆç®—ï¼ˆä½¿ç”¨æ­£ç¢ºçš„ç·¨è¼¯ç›®æ¨™ï¼‰
3. å¢åŠ è¨“ç·´ç›£æ§å’Œè¨ºæ–·
4. å„ªåŒ– learning rate å’Œè¨“ç·´ç­–ç•¥
"""

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, ConcatDataset
import pytorch_lightning as pl
from omegaconf import OmegaConf
from PIL import Image
from torchvision import transforms
import os
import random
import pandas as pd

from diff2flow.trainer_module import TrainerModuleLatentFM


class PseudoEditingDataset(Dataset):
    """Pseudo-Editing Datasetï¼ˆå¼·åŒ–ç‰ˆï¼‰"""

    def __init__(self, data_root, attributes_file=None, noise_level=0.3, drop_ratio=0.5):
        self.data_root = data_root
        self.noise_level = noise_level
        self.drop_ratio = drop_ratio  # æ–°å¢ï¼šå¯èª¿æ•´çš„ drop_ratio
        self.transform = self.get_transform()

        # è‡ªå‹•å°‹æ‰¾ attributes.txt
        if attributes_file is None:
            auto_attr = os.path.join(data_root, "attributes.txt")
            if os.path.exists(auto_attr):
                attributes_file = auto_attr
                print(f"âœ“ è‡ªå‹•æ‰¾åˆ°å±¬æ€§æª”æ¡ˆ: {auto_attr}")

        # è¼‰å…¥è³‡æ–™
        self.images = self.load_images()

        # è¼‰å…¥å±¬æ€§
        if attributes_file and os.path.exists(attributes_file):
            self.attributes = self.load_attributes(attributes_file)
            if self.attributes:
                print(f"âœ“ å±¬æ€§æª”æ¡ˆè¼‰å…¥æˆåŠŸï¼Œå°‡ä½¿ç”¨çœŸå¯¦å±¬æ€§")
            else:
                print(f"âš ï¸  å±¬æ€§æª”æ¡ˆè¼‰å…¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨é è¨­å±¬æ€§")
        else:
            print("âš ï¸  æœªæä¾›å±¬æ€§æª”æ¡ˆï¼Œå°‡ä½¿ç”¨é è¨­ prompts")
            self.attributes = None

    def get_transform(self):
        return transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ])

    def load_images(self):
        """ä½¿ç”¨ os.listdirï¼ˆæ”¯æ´ä¸­æ–‡è·¯å¾‘ï¼‰"""
        image_dir = os.path.join(self.data_root, "images")
        if not os.path.exists(image_dir):
            print(f"âš ï¸  images å­è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ ¹ç›®éŒ„: {self.data_root}")
            image_dir = self.data_root
        else:
            print(f"âœ“ æ‰¾åˆ° images è³‡æ–™å¤¾: {image_dir}")

        valid_extensions = (
            '.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG',
            '.bmp', '.BMP', '.webp', '.WEBP'
        )

        images = []

        try:
            print(f"ğŸ” æœå°‹åœ–ç‰‡åœ¨: {image_dir}")
            all_files = os.listdir(image_dir)

            for filename in all_files:
                if filename.endswith(valid_extensions):
                    full_path = os.path.join(image_dir, filename)
                    if os.path.isfile(full_path):
                        images.append(full_path)

            if len(images) == 0:
                print(f"âš ï¸  åœ¨ {image_dir} æ‰¾ä¸åˆ°åœ–ç‰‡ï¼Œå˜—è©¦æœå°‹å­è³‡æ–™å¤¾...")
                for root, dirs, files in os.walk(image_dir):
                    for filename in files:
                        if filename.endswith(valid_extensions):
                            full_path = os.path.join(root, filename)
                            images.append(full_path)

        except Exception as e:
            print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•è®€å–ç›®éŒ„: {e}")
            return []

        if len(images) == 0:
            print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä»»ä½•åœ–ç‰‡ï¼")
        else:
            print(f"âœ“ ç¸½å…±æ‰¾åˆ° {len(images)} å¼µåœ–ç‰‡")
            for img in images[:3]:
                print(f"   ç¯„ä¾‹: {os.path.basename(img)}")

        return sorted(images)

    def load_attributes(self, attributes_file):
        """è¼‰å…¥å±¬æ€§æª”æ¡ˆä¸¦å»ºç«‹æ˜ å°„"""
        print(f"è¼‰å…¥å±¬æ€§æª”æ¡ˆ: {attributes_file}")

        try:
            with open(attributes_file, 'r') as f:
                first_line = f.readline().strip()
                if first_line.isdigit():
                    skiprows = 1
                else:
                    skiprows = 0

            df = pd.read_csv(attributes_file, delim_whitespace=True, skiprows=skiprows)

            attr_dict = {}
            for idx, row in df.iterrows():
                image_id = str(row.iloc[0])
                image_id_base = os.path.splitext(image_id)[0]

                attrs = {}
                for col in df.columns[1:]:
                    attrs[col] = 1 if row[col] == 1 else 0

                # å„²å­˜å¤šç¨®æ ¼å¼
                attr_dict[image_id] = attrs
                attr_dict[image_id_base] = attrs

                if image_id_base.isdigit():
                    attr_dict[f"{int(image_id_base):05d}"] = attrs

            print(f"âœ“ è¼‰å…¥äº† {len(df)} å€‹åœ–ç‰‡çš„å±¬æ€§ï¼ˆ{len(df.columns)-1} å€‹å±¬æ€§æ¬„ä½ï¼‰")
            return attr_dict

        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥å±¬æ€§æª”æ¡ˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None

    def get_image_attributes(self, image_path):
        if self.attributes is None:
            return self.get_default_attributes()

        filename = os.path.basename(image_path)
        image_id = os.path.splitext(filename)[0]

        possible_ids = [
            image_id,
            filename,
        ]

        if image_id.isdigit():
            possible_ids.append(f"{int(image_id):05d}")

        for pid in possible_ids:
            if pid in self.attributes:
                return self.attributes[pid]

        return self.get_default_attributes()

    def get_default_attributes(self):
        """é è¨­å±¬æ€§ï¼ˆç•¶æ‰¾ä¸åˆ°æ™‚ï¼‰"""
        return {
            'Male': 0, 'Young': 1, 'Smiling': 1, 'Eyeglasses': 0,
            'Black_Hair': 1, 'Blond_Hair': 0, 'Brown_Hair': 0,
            'Wearing_Hat': 0, 'Wearing_Earrings': 0, 'Heavy_Makeup': 0
        }

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image_path = self.images[idx]

        image = Image.open(image_path).convert('RGB')
        image_tensor = self.transform(image)

        attributes = self.get_image_attributes(image_path)

        # Source: åŠ é›œè¨Š
        noise = torch.randn_like(image_tensor) * self.noise_level
        source_tensor = torch.clamp(image_tensor + noise, -1, 1)

        # Target: åŸåœ–
        target_tensor = image_tensor

        # Prompts: Source ä½¿ç”¨é«˜ drop_ratioï¼ŒTarget ä½¿ç”¨ 0
        source_prompt = self.generate_prompt(attributes, drop_ratio=self.drop_ratio)
        target_prompt = self.generate_prompt(attributes, drop_ratio=0.0)

        return {
            'source_image': source_tensor,
            'target_image': target_tensor,
            'source_prompt': source_prompt,
            'target_prompt': target_prompt
        }

    def generate_prompt(self, attributes, drop_ratio=0.0):
        """
        ç”Ÿæˆ promptï¼ˆå¼·åŒ–ç‰ˆ - åŒ…å«æ›´å¤šå±¬æ€§ï¼‰

        drop_ratio: ä¸Ÿæ£„å±¬æ€§çš„æ©Ÿç‡
        - 0.0: ä¿ç•™æ‰€æœ‰å±¬æ€§ï¼ˆTargetï¼‰
        - 0.5: ä¸Ÿæ£„ 50% å±¬æ€§ï¼ˆSourceï¼‰
        """
        # åŸºç¤æè¿°
        gender = 'man' if attributes.get('Male', 0) == 1 else 'woman'
        age = 'young' if attributes.get('Young', 1) == 1 else 'old'
        prompt = f"a photo of a {age} {gender}"

        features = []

        # 1. é«®è‰²ï¼ˆé‡è¦ï¼äº’æ–¥å±¬æ€§ï¼‰
        hair_attrs = [
            ('Black_Hair', 'black hair'),
            ('Blond_Hair', 'blond hair'),
            ('Brown_Hair', 'brown hair'),
            ('Gray_Hair', 'gray hair')
        ]
        for attr, desc in hair_attrs:
            if attributes.get(attr, 0) == 1:
                if random.random() > drop_ratio:
                    features.append(desc)
                break  # åªå–ä¸€ç¨®é«®è‰²

        # 2. è‡‰éƒ¨ç‰¹å¾µï¼ˆé‡è¦ï¼ï¼‰
        facial_features = [
            ('Smiling', 'smiling'),
            ('Mouth_Slightly_Open', 'mouth slightly open'),
            ('High_Cheekbones', 'high cheekbones'),
            ('Big_Nose', 'big nose'),
            ('Pointy_Nose', 'pointy nose'),
            ('Chubby', 'chubby face'),
            ('Oval_Face', 'oval face')
        ]
        for attr, desc in facial_features:
            if attributes.get(attr, 0) == 1:
                if random.random() > drop_ratio:
                    features.append(desc)

        # 3. åŒ–å¦å’Œçš®è†š
        makeup_features = [
            ('Heavy_Makeup', 'wearing heavy makeup'),
            ('Wearing_Lipstick', 'wearing lipstick'),
            ('Rosy_Cheeks', 'rosy cheeks'),
            ('Pale_Skin', 'pale skin')
        ]
        for attr, desc in makeup_features:
            if attributes.get(attr, 0) == 1:
                if random.random() > drop_ratio:
                    features.append(desc)

        # 4. é…ä»¶ï¼ˆæœ€é‡è¦ï¼ç·¨è¼¯æ•ˆæœæ˜é¡¯ï¼‰
        accessories = []
        accessory_attrs = [
            ('Eyeglasses', 'eyeglasses'),
            ('Wearing_Hat', 'hat'),
            ('Wearing_Earrings', 'earrings'),
            ('Wearing_Necklace', 'necklace'),
            ('Wearing_Necktie', 'necktie')
        ]
        for attr, desc in accessory_attrs:
            if attributes.get(attr, 0) == 1:
                if random.random() > drop_ratio:
                    accessories.append(desc)

        # 5. é¬å­ç›¸é—œï¼ˆç”·æ€§ï¼‰
        if attributes.get('Male', 0) == 1:
            beard_features = [
                ('Mustache', 'mustache'),
                ('Goatee', 'goatee'),
                ('Sideburns', 'sideburns'),
                ('5_o_Clock_Shadow', '5 o\'clock shadow')
            ]
            for attr, desc in beard_features:
                if attributes.get(attr, 0) == 1:
                    if random.random() > drop_ratio:
                        features.append(desc)

        # çµ„åˆç‰¹å¾µ
        if features:
            prompt += ", " + ", ".join(features)

        # çµ„åˆé…ä»¶
        if accessories:
            prompt += ", wearing " + " and ".join(accessories)

        return prompt


class EditingFineTuner(pl.LightningModule):
    """ç·¨è¼¯å¾®èª¿æ¨¡çµ„ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""

    def __init__(self, pretrained_ckpt, config, learning_rate=1e-5):
        super().__init__()
        self.save_hyperparameters(ignore=['model'])

        print(f"æ­£åœ¨è¼‰å…¥ Checkpoint: {pretrained_ckpt}")
        self.model = TrainerModuleLatentFM.load_from_checkpoint(
            pretrained_ckpt,
            map_location='cpu',
            **self.get_model_kwargs(config)
        )

        # è‡ªå‹•åµæ¸¬ Text Encoder
        self.text_encoder = self._find_text_encoder()

        self.learning_rate = learning_rate
        self.automatic_optimization = False

        # è¨˜éŒ„çµ±è¨ˆ
        self.v_norm_history = []
        self.prompt_same_count = 0
        self.prompt_total_count = 0

    def _find_text_encoder(self):
        """è‡ªå‹•åµæ¸¬ Text Encoder"""
        print("ğŸ” æ­£åœ¨åµæ¸¬ Text Encoder...")

        if hasattr(self.model, 'cond_stage'):
            print(f"âœ… æ‰¾åˆ° self.model.cond_stage")
            return self.model.cond_stage

        candidates = ['cond_stage_model', 'conditioner', 'text_encoder']
        for name in candidates:
            if hasattr(self.model, name):
                print(f"âœ… æ‰¾åˆ° self.model.{name}")
                return getattr(self.model, name)

        if hasattr(self.model, 'model'):
            for name in ['cond_stage', 'cond_stage_model']:
                if hasattr(self.model.model, name):
                    print(f"âœ… æ‰¾åˆ° self.model.model.{name}")
                    return getattr(self.model.model, name)

        print("âŒ è­¦å‘Šï¼šç„¡æ³•è‡ªå‹•åµæ¸¬ Text Encoder")
        return None

    def encode_text(self, text_list):
        """çµ±ä¸€çš„æ–‡å­—ç·¨ç¢¼ä»‹é¢"""
        if self.text_encoder is None:
            raise RuntimeError("Text Encoder æœªæ‰¾åˆ°ï¼")

        if hasattr(self.text_encoder, 'encode'):
            return self.text_encoder.encode(text_list)

        return self.text_encoder(text_list)

    def training_step(self, batch, batch_idx):
        opt = self.optimizers()

        source_img = batch['source_image']
        target_img = batch['target_image']
        source_prompt = batch['source_prompt']
        target_prompt = batch['target_prompt']

        # çµ±è¨ˆ Prompt ç›¸åŒç‡
        for sp, tp in zip(source_prompt, target_prompt):
            self.prompt_total_count += 1
            if sp == tp:
                self.prompt_same_count += 1

        # æ¯ 100 æ­¥è¼¸å‡ºçµ±è¨ˆ
        if batch_idx % 100 == 0 and self.prompt_total_count > 0:
            same_ratio = self.prompt_same_count / self.prompt_total_count
            print(f"\n[Step {batch_idx}] Prompt ç›¸åŒç‡: {same_ratio*100:.1f}%")
            if same_ratio > 0.3:
                print(f"âš ï¸  è­¦å‘Šï¼šPrompt ç›¸åŒç‡éé«˜ï¼æ‡‰è©²æ¥è¿‘ 0%")

        with torch.no_grad():
            z_source = self.model.encode_first_stage(source_img)
            z_target = self.model.encode_first_stage(target_img)
            c_target = self.encode_text(target_prompt)

        # Flow Matching
        t = torch.rand(z_source.shape[0], device=self.device)

        # âš ï¸ é—œéµï¼šé€™è£¡æ˜¯ç·¨è¼¯çš„æ ¸å¿ƒ
        # z_t æ˜¯å¾ source åˆ° target çš„æ’å€¼
        z_t = (1 - t[:, None, None, None]) * z_source + t[:, None, None, None] * z_target

        # v_target æ˜¯ã€Œå¾ source åˆ° targetã€çš„é€Ÿåº¦å ´
        v_target = z_target - z_source

        # æ¨¡å‹é æ¸¬ï¼šçµ¦å®šç•¶å‰ç‹€æ…‹ z_t å’Œç›®æ¨™ promptï¼Œé æ¸¬æ‡‰è©²å¾€å“ªå€‹æ–¹å‘èµ°
        v_pred = self.model.model(
            z_t,
            t,
            context=z_source,      # Identity preservation
            context_ca=c_target    # Target prompt guidance
        )

        # Loss
        loss = F.mse_loss(v_pred, v_target)

        # å„ªåŒ–
        opt.zero_grad()
        self.manual_backward(loss)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        opt.step()

        # è¨˜éŒ„
        v_norm = v_pred.norm().item()
        self.v_norm_history.append(v_norm)

        # æ¯ 50 æ­¥è¨ˆç®—å¹³å‡
        if batch_idx % 50 == 0 and len(self.v_norm_history) > 0:
            avg_v_norm = sum(self.v_norm_history[-50:]) / min(50, len(self.v_norm_history))
            print(f"[Step {batch_idx}] avg_v_norm (last 50): {avg_v_norm:.2f}")

            if avg_v_norm > 50:
                print(f"âš ï¸  v_norm åé«˜ï¼")
            elif avg_v_norm < 15:
                print(f"âœ“ v_norm è‰¯å¥½")

        self.log('train_loss', loss, prog_bar=True)
        self.log('v_norm', v_norm, prog_bar=True)

        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=0.01
        )

        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.trainer.max_epochs,
            eta_min=1e-7
        )

        return [optimizer], [scheduler]

    @staticmethod
    def get_model_kwargs(config):
        return {
            'fm_cfg': config.model.fm_cfg,
            'first_stage': config.autoencoder,
            'cond_stage_cfg': config.task.cond_stage_cfg,
            'lora_cfg': config.lora.lora_cfg,
            'start_from_noise': config.model.start_from_noise,
            'noising_step': config.model.noising_step,
            'scale_factor': config.autoencoder.get('scale_factor', 0.18215)
        }


def train_editing_finetuner(
    pretrained_ckpt,
    config_path,
    train_data_root,
    val_data_root=None,
    train_attributes_file=None,
    val_attributes_file=None,
    output_dir="./editing_finetuned",
    use_combined_data=False,
    max_epochs=30,
    batch_size=4,
    learning_rate=1e-5,
    noise_level=0.3,
    drop_ratio=0.5  # æ–°å¢ï¼šå¯èª¿æ•´çš„ drop_ratio
):
    """åŸ·è¡Œç·¨è¼¯å¾®èª¿"""

    config = OmegaConf.load(config_path)

    print("="*60)
    print("è¨“ç·´é…ç½®")
    print("="*60)
    print(f"  Noise Level: {noise_level}")
    print(f"  Drop Ratio: {drop_ratio}")
    print(f"  Learning Rate: {learning_rate}")
    print(f"  Batch Size: {batch_size}")
    print(f"  Max Epochs: {max_epochs}")
    print("="*60 + "\n")

    print("ä½¿ç”¨ Pseudo-Editing è¨“ç·´...")
    train_dataset = PseudoEditingDataset(
        train_data_root,
        train_attributes_file,
        noise_level=noise_level,
        drop_ratio=drop_ratio
    )

    print(f"è¨“ç·´è³‡æ–™: {len(train_dataset)} å¼µ")

    if len(train_dataset) == 0:
        print("âŒ è¨“ç·´è³‡æ–™ç‚ºç©ºï¼Œçµ‚æ­¢åŸ·è¡Œ")
        return

    # æ¸¬è©¦å‰ 3 å€‹æ¨£æœ¬
    print("\n" + "="*60)
    print("æ¸¬è©¦å‰ 3 å€‹è¨“ç·´æ¨£æœ¬")
    print("="*60)
    for i in range(min(3, len(train_dataset))):
        sample = train_dataset[i]
        print(f"\n[æ¨£æœ¬ {i+1}]")
        print(f"  Source: {sample['source_prompt']}")
        print(f"  Target: {sample['target_prompt']}")
        print(f"  ç›¸åŒ: {'âŒ æœ‰å•é¡Œï¼' if sample['source_prompt'] == sample['target_prompt'] else 'âœ“'}")
    print("="*60 + "\n")

    val_dataset = None
    if use_combined_data:
        print("âš ï¸  ä½¿ç”¨åˆä½µæ¨¡å¼ï¼štrain+val ä¸€èµ·è¨“ç·´")
        if val_data_root:
            val_part = PseudoEditingDataset(
                val_data_root,
                val_attributes_file,
                noise_level=noise_level,
                drop_ratio=drop_ratio
            )
            train_dataset = ConcatDataset([train_dataset, val_part])
            print(f"åˆä½µå¾Œè¨“ç·´è³‡æ–™: {len(train_dataset)} å¼µ")
    elif val_data_root:
        print(f"é©—è­‰è³‡æ–™: {val_data_root}")
        val_dataset = PseudoEditingDataset(
            val_data_root,
            val_attributes_file,
            noise_level=noise_level,
            drop_ratio=drop_ratio
        )
        print(f"é©—è­‰è³‡æ–™: {len(val_dataset)} å¼µ")

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )

    val_dataloader = None
    if val_dataset and len(val_dataset) > 0:
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )

    model = EditingFineTuner(
        pretrained_ckpt=pretrained_ckpt,
        config=config,
        learning_rate=learning_rate
    )

    trainer = pl.Trainer(
        max_epochs=max_epochs,
        accelerator='gpu',
        devices=1,
        default_root_dir=output_dir,
        log_every_n_steps=10,
        val_check_interval=0.5 if val_dataloader else None,
        callbacks=[
            pl.callbacks.ModelCheckpoint(
                dirpath=f"{output_dir}/checkpoints",
                filename='editing-{epoch:02d}-{train_loss:.4f}',
                save_top_k=3,
                monitor='train_loss',
                mode='min',
                save_last=True,
                every_n_epochs=5
            ),
            pl.callbacks.LearningRateMonitor(logging_interval='step')
        ]
    )

    print("\nğŸš€ é–‹å§‹å¾®èª¿è¨“ç·´...")
    if val_dataloader:
        trainer.fit(model, train_dataloader, val_dataloader)
    else:
        trainer.fit(model, train_dataloader)

    print(f"\nâœ“ è¨“ç·´å®Œæˆï¼")
    print(f"Checkpoints å„²å­˜åœ¨: {output_dir}/checkpoints/")


if __name__ == "__main__":
    # è¨­å®š
    PRETRAINED_CKPT = "/home/cchen/æ¡Œé¢/diff2flow-smalldata (å‰¯æœ¬)/logs/celeba_identity_v1_2026-01-12-19-17-48/checkpoints/last.ckpt"
    CONFIG_PATH = "/home/cchen/æ¡Œé¢/diff2flow-smalldata (å‰¯æœ¬)/logs/celeba_identity_v1_2026-01-12-19-17-48/config.yaml"

    # è³‡æ–™è·¯å¾‘
    TRAIN_DATA = "/home/cchen/æ¡Œé¢/MM-CelebA-HQ-Dataset-main\split/train"
    VAL_DATA = "/home/cchen/æ¡Œé¢/MM-CelebA-HQ-Dataset-main\split/val"

    # å±¬æ€§æª”æ¡ˆï¼ˆè¨­æˆ None è‡ªå‹•æ‰¾ï¼‰
    TRAIN_ATTRIBUTES = None
    VAL_ATTRIBUTES = None

    OUTPUT_DIR = "/home/cchen/æ¡Œé¢/diff2flow-smalldata (å‰¯æœ¬)/editing_finetuned"

    # ==========================================
    # è¨“ç·´åƒæ•¸ï¼ˆé‡è¦ï¼ï¼‰
    # ==========================================
    MAX_EPOCHS = 30          # è¨“ç·´è¼ªæ•¸
    BATCH_SIZE = 4           # æ‰¹æ¬¡å¤§å°
    LEARNING_RATE = 2e-5     # å­¸ç¿’ç‡ï¼ˆæé«˜äº†ï¼ï¼‰
    NOISE_LEVEL = 0.3        # é›œè¨Šå¼·åº¦
    DROP_RATIO = 0.5         # Prompt ä¸Ÿæ£„ç‡ï¼ˆæé«˜äº†ï¼ï¼‰

    # è¨“ç·´æ¨¡å¼
    USE_STANDARD_SPLIT = True
    USE_COMBINED = False

    if USE_STANDARD_SPLIT:
        print("\n" + "="*60)
        print("ä½¿ç”¨æ¨™æº–åˆ†å‰²æ¨¡å¼")
        print(f"  è¨“ç·´: {TRAIN_DATA}")
        print(f"  é©—è­‰: {VAL_DATA}")
        print("="*60 + "\n")

        train_editing_finetuner(
            pretrained_ckpt=PRETRAINED_CKPT,
            config_path=CONFIG_PATH,
            train_data_root=TRAIN_DATA,
            val_data_root=VAL_DATA,
            train_attributes_file=TRAIN_ATTRIBUTES,
            val_attributes_file=VAL_ATTRIBUTES,
            output_dir=OUTPUT_DIR,
            use_combined_data=False,
            max_epochs=MAX_EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE,
            noise_level=NOISE_LEVEL,
            drop_ratio=DROP_RATIO
        )

    elif USE_COMBINED:
        print("\n" + "="*60)
        print("ä½¿ç”¨åˆä½µæ¨¡å¼ï¼ˆtrain + valï¼‰")
        print(f"  è¨“ç·´: {TRAIN_DATA} + {VAL_DATA}")
        print("="*60 + "\n")

        train_editing_finetuner(
            pretrained_ckpt=PRETRAINED_CKPT,
            config_path=CONFIG_PATH,
            train_data_root=TRAIN_DATA,
            val_data_root=VAL_DATA,
            train_attributes_file=TRAIN_ATTRIBUTES,
            val_attributes_file=VAL_ATTRIBUTES,
            output_dir=OUTPUT_DIR,
            use_combined_data=True,
            max_epochs=MAX_EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE,
            noise_level=NOISE_LEVEL,
            drop_ratio=DROP_RATIO
        )

    print("\n" + "="*60)
    print("å¾®èª¿å®Œæˆï¼")
    print(f"Checkpoints: {OUTPUT_DIR}/checkpoints/")
    print("="*60)